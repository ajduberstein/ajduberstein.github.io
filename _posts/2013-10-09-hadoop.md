---
layout: page
title : What I Wish Someone Had Told Me About Hadoop
category : Business
tagline : "Had I had a big data sherpa, here's what he would have said."
image: http://hadoop.apache.org/images/hadoop-logo.jpg
tags : [ Hadoop, Hive, analytics]
---
{% include JB/setup %}

**Philosophical guidance**
&nbsp;
* Prototype in Impala/Hive; implement in Pig. Starting out on Hadoop, I thought I'd only have to use Hive with an occasional Java user-defined function. It turns out I've done the majority of my analysis in Pig and used Hive really for ad hoc analysis and rapid insights. Hive saves developer time&#8212;it's intuitive, given that we analysts know SQL—but if you have to re-run code a bunch and you want it to run blazingly fast, use Pig and its built-in optimizer.
* Hadoop is often the best place to start, but it’s not always the best place to finish. If you've pared down your data to megabytes and you’re doing ad hoc work, pull it down to your machine and use R, SAS, or Excel. This blog post provides a great story and some [heuristics](http://www.chrisstucchio.com/blog/2013/hadoop_hatred.html)

**Code tips**

**All MapReduce task languages**
&nbsp;

* If you know a little about MapReduce, I’d recommend [this page as a starting point](https://github.com/twitter/scalding/wiki/Rosetta-Code). Distributed Grep and word count are something I use all the time.
* Pig and Hive are actually writing Java MapReduce jobs in the background. What Java is to machine code, Pig and Hive are to Java MapReduce jobs.
* Know how MapReduce works. Have a basic grasp on an object-oriented or imperative programming language like Java, the C-family, Python, or Ruby (rather than a declarative language like SAS or SQL) and try running a MapReduce task from scratch. Knowing what Hive and Pig are doing in the back-end helps you understand how to write efficient code.
* Skim at least the first chapter of O'Reilly's *Hadoop: The Definitive Guide*. 
* Google is forever your ally.

**Hive**
&nbsp;
- There’s no semi-join with the `IN` syntax like in Oracle's PL/SQL (e.g., `WHERE store_code IN (‘111’,’214’,’318’,’924’,’525’)`)  but there is a `LEFT SEMI JOIN` that’s close.
- Common Table Expressions aren’t currently supported. Use subqueries or create/drop tables.
- `CREATE EXTERNAL TABLE` is the best way to transfer data from Pig to Hive.
- `CREATE TABLE AS SELECT` (also called CTAS) is the best way to transfer data from Hive to Pig.
- `SELECT * FROM table_name LIMIT 10` is the way to display ten rows, much like Oracle SQL’s ROWNUM <= 10.
- `ORDER BY` and `JOIN` are often processing bottlenecks. For the former, Hive has `ORDER BY`, `SORT BY`, `DISTRIBUTE BY`, and `CLUSTER BY`, which are related to MapReduce workflow and can speed your queries up if used well, as explained [here]( https://cwiki.apache.org/confluence/display/Hive/LanguageManual+SortBy). There’s a similar page for [joins](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Joins).
- Hive error messages are not helpful. Like a good programmer, Google the message.

**Pig**
&nbsp;
- Pig is genuinely incredible. It's my favorite component of the Hadoop ecosystem. The `SPLIT` statement is great for observing outliers, and the ability to easily bifurcate output is something I miss when I go back to SQL.
- `ILLUSTRATE`, `DUMP`, `LIMIT`, `DESCRIBE` and `SAMPLE` are necessary for debugging your Pig code. Read the [Apache Pig documentation](http://pig.apache.org/docs/r0.11.1/func.html).
- `FOREACH var_name *nested_block*` is how you do a `COUNT(DISTINCT variable)` in Pig.
- O'Reilly's *Programming Pig* is the best resource I’ve encountered, available free [here](http://chimera.labs.oreilly.com/books/1234000001811/index.html).

**Impala**
&nbsp;
- It really is blazingly fast, but it's also young: As of this post, Impala's roughly a year old. This immaturity means it won't meet all your needs yet. There's no DDL currently&#8212;tables must be created in Hive.
- When you do want to create a table, declare it in Hive, use statement `INVALIDATE METADATA` and `REFRESH table_name`.
